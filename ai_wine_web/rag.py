# rag.py
import io, os, json, glob
import numpy as np
import soundfile as sf
import faiss
from openai import OpenAI
import re

# -------------------------
# API KEYï¼ˆåªå…è¨± envï¼‰
# -------------------------
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY is not set")

client = OpenAI(api_key=OPENAI_API_KEY)

# -------------------------
# Models
# -------------------------
CHAT_MODEL = "gpt-4o-mini"
EMBED_MODEL = "text-embedding-3-small"
STT_MODEL = "gpt-4o-transcribe"
TTS_MODEL = "gpt-4o-mini-tts"
TTS_VOICE = "alloy"
# alloy      â†’ ä¸­æ€§ AI è²
# verse      â†’ æ¯”è¼ƒè‡ªç„¶æ•˜äº‹æ„Ÿ
# aria       â†’ åå¥³æ€§
# sage       â†’ åæˆç†Ÿæ²‰ç©©

# -------------------------
# Paths
# -------------------------
BASE_DIR = os.path.dirname(__file__)
KB_DIR = os.path.join(BASE_DIR, "kb")
INDEX_PATH = os.path.join(BASE_DIR, "kb.index")
META_PATH = os.path.join(BASE_DIR, "kb_texts.json")

class StreamingTextChunker:

    def __init__(self, min_chars=20):
        self.buffer = ""
        self.min_chars = min_chars

    def push(self, text):

        self.buffer += text

        # å„ªå…ˆæ‰¾ä¸­æ–‡åœé “ç¬¦
        split_chars = "ï¼Œã€‚ï¼ï¼Ÿ,.!? "

        if len(self.buffer) < self.min_chars:
            return None

        for i, ch in enumerate(self.buffer):
            if ch in split_chars and i >= self.min_chars:
                chunk = self.buffer[:i+1]
                self.buffer = self.buffer[i+1:]
                return chunk

        return None

    def flush(self):
        if self.buffer:
            chunk = self.buffer
            self.buffer = ""
            return chunk
        return None
    
def pipeline_tts_stream(text):

    chunker = StreamingTextChunker()

    # æ¨¡æ“¬ GPT streamingï¼ˆç›®å‰å…ˆç”¨æ•´æ®µæ–‡å­—ï¼‰
    for token in text:

        chunk = chunker.push(token)

        if chunk:
            print("ğŸ¤ TTS chunk:", chunk)

            audio = text_to_speech_wav_bytes(chunk)
            yield audio

    # flush
    last = chunker.flush()
    if last:
        yield text_to_speech_wav_bytes(last)
   
# -------------------------
# STT
# -------------------------
def speech_to_text(wav_bytes: bytes) -> str:
    try:
        data, sr = sf.read(io.BytesIO(wav_bytes), dtype="float32")
    except Exception as e:
        raise RuntimeError(f"Invalid audio input: {e}")

    # mono
    if data.ndim > 1:
        data = np.mean(data, axis=1)

    # resample if needed
    if sr != 16000:
        try:
            import resampy
            data = resampy.resample(data, sr, 16000)
            sr = 16000
        except ImportError:
            # fallbackï¼šä¸ resampleï¼Œè‡³å°‘ä¸ç‚¸
            pass

    buf = io.BytesIO()
    sf.write(buf, data, sr, format="WAV", subtype="PCM_16")
    buf.seek(0)
    buf.name = "speech.wav"

    result = client.audio.transcriptions.create(
        model=STT_MODEL,
        file=buf
    )
    return (result.text or "").strip()

# -------------------------
# TTS
# -------------------------
def text_to_speech_wav_bytes(text: str) -> bytes:
    audio_response = client.audio.speech.create(
        model=TTS_MODEL,
        voice=TTS_VOICE,
        input=text,
        speed=1.05
    )
    return audio_response.read()

# -------------------------
# KB
# -------------------------
def load_docs(chunk_size=300):
    docs = []
    for path in glob.glob(os.path.join(KB_DIR, "*.txt")):
        with open(path, "r", encoding="utf-8") as f:
            text = f.read()
        for p in text.split("\n\n"):
            p = p.strip()
            if p:
                docs.append(p[:chunk_size])
    return docs

def build_index():
    if os.path.exists(INDEX_PATH) and os.path.exists(META_PATH):
        index = faiss.read_index(INDEX_PATH)
        texts = json.load(open(META_PATH, encoding="utf-8"))
        return index, texts

    texts = load_docs()
    if not texts:
        raise RuntimeError(f"KB è³‡æ–™å¤¾ '{KB_DIR}' è£¡æ²’æœ‰ä»»ä½• .txt æ–‡ä»¶")

    emb = client.embeddings.create(model=EMBED_MODEL, input=texts)
    vectors = np.array([e.embedding for e in emb.data], dtype=np.float32)
    faiss.normalize_L2(vectors)

    index = faiss.IndexFlatIP(vectors.shape[1])
    index.add(vectors)

    faiss.write_index(index, INDEX_PATH)
    json.dump(texts, open(META_PATH, "w", encoding="utf-8"), ensure_ascii=False)

    return index, texts

def rag_search(query, index, texts, k=2):
    q_emb = client.embeddings.create(model=EMBED_MODEL, input=[query])
    qv = np.array(q_emb.data[0].embedding, dtype=np.float32).reshape(1, -1)
    faiss.normalize_L2(qv)
    _, ids = index.search(qv, k)
    return "\n\n".join(texts[i] for i in ids[0] if i >= 0)

# -------------------------
# GPT
# -------------------------
def ask_gpt(question, context):
    system_prompt = (
        "ä½ æ˜¯ AI ç´…é…’æ«ƒèªéŸ³åŠ©ç†ï¼Œå°ˆé–€ä»‹ç´¹å…¨å®¶çš„è‘¡è„é…’ã€‚"
        "è«‹ç”¨ç¹é«”ä¸­æ–‡ï¼Œä½†é…’åç”¨è‹±æ–‡ï¼Œèªæ°£å°ˆæ¥­è¦ªåˆ‡ï¼Œé©åˆèªéŸ³æ’­æ”¾ã€‚"
    )

    resp = client.responses.create(
        model=CHAT_MODEL,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"{question}\n\n{context}"}
        ]
    )
    return (resp.output_text or "").strip()
